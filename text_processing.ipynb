{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\soura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "## DataSets Considerd:\n",
    "### 1. emoji\n",
    "### 2. emotion\n",
    "### 3. sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    new_text = []\n",
    "    for t in text.split(\" \"):\n",
    "        t = 'user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        t = \"num\" if t.isnumeric() else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_doc(doc):\n",
    "    # split into tokens by white space\n",
    "    rtr = []\n",
    "    for tokens in doc:\n",
    "        x= tokens\n",
    "        tokens = preprocess(tokens)\n",
    "        tokens = tokens.split()\n",
    "        # remove punctuatio n from each token\n",
    "        tokens = [word.lower() for word in tokens]\n",
    "        table = str.maketrans('', '', punctuation)\n",
    "        tokens = [w.translate(table) for w in tokens]\n",
    "        # remove remaining tokens that are not alphabetic\n",
    "        tokens = [word for word in tokens if word.isalpha()]\n",
    "        # filter out stop words\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [w for w in tokens if not w in stop_words]\n",
    "        # filter out short tokens\n",
    "        tokens = [word for word in tokens if len(word) > 1]\n",
    "        if len(tokens) > 0:\n",
    "            rtr.append(\" \".join(tokens))\n",
    "        else: rtr.append(x)\n",
    "    return rtr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making directory to store processed data\n",
    "os.mkdir(\"proccessed_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"datasets/\"\n",
    "dir_names = [\"emoji\",\"emotion\",\"sentiment\"]\n",
    "text_files = ['test_text.txt','train_text.txt','val_text.txt']\n",
    "label_files = ['test_labels.txt','train_labels.txt','val_labels.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for i in range(len(dir_names)):  # iterating through choosen directories\n",
    "    # storing current working directory in dir_\n",
    "    dir_ = file_path + dir_names[i]\n",
    "    # Finding all the text files in cwd\n",
    "    file_list = os.listdir(dir_)\n",
    "    # Iterating through all present files in cwd\n",
    "    for file in file_list:\n",
    "        idx=[]\n",
    "        # If current file is data then we need to preprocess it\n",
    "        if file in text_files:\n",
    "            # storing cwd\n",
    "            file_ = dir_+\"/\"+file\n",
    "            # List to store raw data\n",
    "            raw_text = []\n",
    "            #opening text file \n",
    "            with open(file_,encoding=\"utf8\") as fd:\n",
    "                # iterating through text\n",
    "                for line in fd:\n",
    "                    raw_text.append(line.strip())\n",
    "            # Preproccessing the data\n",
    "            processed_text = clean_doc(raw_text)\n",
    "            #print(len(processed_text),len(raw_text))\n",
    "            # path to store proccessed data\n",
    "            path_ = \"proccessed_data/\"+dir_names[i]\n",
    "            \n",
    "            # path to stored proccessed data\n",
    "            csv_file = path_+\"/\"+file[:-4]+\".csv\"\n",
    "            # if proccessed_data directory is present\n",
    "            if os.path.exists(path_):\n",
    "                # saving file as csv file\n",
    "                pd.DataFrame(processed_text,columns=[\"text\"]).to_csv(csv_file,index=None)\n",
    "            \n",
    "            else:\n",
    "                # if path is not present create a directory\n",
    "                os.mkdir(path_)\n",
    "                # saving csv file\n",
    "                pd.DataFrame(processed_text,columns=[\"text\"]).to_csv(csv_file,index=None)\n",
    "                \n",
    "        # if current file is labels we need not to proces it\n",
    "        elif file in label_files:\n",
    "            file_ = dir_+\"/\"+file\n",
    "            labels = []\n",
    "            with open(file_,encoding=\"utf8\") as fd:\n",
    "                for line in fd:\n",
    "                    labels.append(line.strip())\n",
    "            path_ = \"proccessed_data/\"+dir_names[i]\n",
    "            csv_file = path_+\"/\"+file[:-4]+\".csv\"\n",
    "            if os.path.exists(path_):\n",
    "                labels = pd.DataFrame(labels,columns=[\"text\"])\n",
    "                labels.to_csv(csv_file,index=None)\n",
    "                \n",
    "            else :\n",
    "                os.mkdir(path_)\n",
    "                labels = pd.DataFrame(labels,columns=[\"text\"])\n",
    "                labels.to_csv(csv_file,index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
